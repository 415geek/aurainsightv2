import os
import json
import math
import pandas as pd
import streamlit as st
from datetime import datetime, timedelta
import requests
import pandas as pd
import nltk
import time
import concurrent.futures
import io

# Ensure TextBlob corpora are downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')
try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger')
try:
    nltk.data.find('corpora/brown')
except LookupError:
    nltk.download('brown')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

from meteostat import Point, Daily
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from openai import OpenAI
from textblob import TextBlob
import concurrent.futures
import re
import io

# ============================
# FEATURE FLAGS & CONFIG (v2.1)
# ============================
ENABLE_DATA_UPLOAD_PIPELINE = os.getenv("ENABLE_DATA_UPLOAD_PIPELINE", "true").lower() == "true"
# æ¨¡å¼ï¼šprompt_bound (ä½¿ç”¨ OpenAI åå°ç»‘å®šçš„æ¨¡å‹) | explicit_model (æ˜¾å¼æŒ‡å®š ID)
AURAINSIGHT_MODEL_MODE = os.getenv("AURAINSIGHT_MODEL_MODE", "prompt_bound")
AURAINSIGHT_MODEL_ID = os.getenv("AURAINSIGHT_MODEL_ID", "gpt-4o")
AURAINSIGHT_PROMPT_ID = os.getenv("AURAINSIGHT_PROMPT_ID", "pmpt_6971b3bd094081959997af7730098d45020d02ec1efab62b")

# ============================
# DATA PIPELINE (CANONICAL SCHEMA v2.1)
# ============================
class DataPipeline:
    COLUMN_MAP = {
        'æ—¥æœŸ': 'date', 'date': 'date', 'æ—¶é—´': 'date', 'day': 'date',
        'è®¢å•é‡': 'orders', 'å•é‡': 'orders', 'orders': 'orders', 'order_count': 'orders',
        'è¥æ”¶': 'revenue', 'å®æ”¶': 'revenue', 'revenue': 'revenue', 'sales': 'revenue', 'é‡‘é¢': 'revenue',
        'æ¸ é“': 'channel', 'æ¥æº': 'channel', 'channel': 'channel', 'platform': 'channel',
        'å–æ¶ˆç‡': 'cancel_rate', 'é€€å•ç‡': 'cancel_rate',
        'å¤‡é¤æ—¶é—´': 'prep_time', 'å‡ºé¤æ—¶é—´': 'prep_time'
    }

    @classmethod
    def parse_file(cls, uploaded_file):
        fname = uploaded_file.name
        ext = fname.split('.')[-1].lower()
        
        try:
            if ext in ['csv']:
                df = pd.read_csv(uploaded_file)
            elif ext in ['xlsx', 'xls']:
                df = pd.read_excel(uploaded_file)
            elif ext in ['pdf']:
                from PyPDF2 import PdfReader
                reader = PdfReader(uploaded_file)
                text = "".join([p.extract_text() for p in reader.pages])
                return {"type": "text", "content": text, "source": fname}
            elif ext in ['docx']:
                import docx
                doc = docx.Document(uploaded_file)
                text = "\n".join([p.text for p in doc.paragraphs])
                return {"type": "text", "content": text, "source": fname}
            elif ext in ['png', 'jpg', 'jpeg', 'webp']:
                return {"type": "image_placeholder", "content": "OCR Pending v2.2", "source": fname}
            else:
                return {"error": f"ä¸æ”¯æŒçš„æ ¼å¼: {ext}"}
            
            # åŸºç¡€åˆ—åæ˜ å°„
            df = df.rename(columns=lambda x: cls.COLUMN_MAP.get(str(x).lower().strip(), x))
            return {"type": "table", "data": df, "source": fname}
        except Exception as e:
            return {"error": f"è§£æå¤±è´¥ ({fname}): {str(e)}"}

    @classmethod
    def build_canonical_schema(cls, files):
        operational_data = {
            "time_series": [],
            "channels_summary": [],
            "data_quality_report": {"missing_fields": [], "warnings": []},
            "traceability": []
        }
        
        all_dfs = []
        for f in files:
            res = cls.parse_file(f)
            if "error" in res:
                operational_data["data_quality_report"]["warnings"].append(res["error"])
                continue
            if res["type"] == "table":
                df = res["data"]
                # å°è¯•æ—¥æœŸæ ‡å‡†åŒ–
                if 'date' in df.columns:
                    try:
                        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
                    except: pass
                
                # æ¸…æ´—æ•°å€¼
                for col in ['orders', 'revenue']:
                    if col in df.columns:
                        df[col] = df[col].apply(cls.clean_numeric)
                
                all_dfs.append({"df": df, "source": res["source"]})

        if not all_dfs:
            return operational_data

        # åˆå¹¶æ•°æ®
        merged = pd.concat([d["df"] for d in all_dfs], ignore_index=True)
        
        # è®¡ç®— AOV (DERIVED)
        if 'revenue' in merged.columns and 'orders' in merged.columns:
            merged['aov'] = (merged['revenue'] / merged['orders']).replace([float('inf'), -float('inf')], 0).fillna(0)
            operational_data["traceability"].append({"field": "aov", "source": "derived: revenue/orders", "tag": "DERIVED_DATA"})

        # ç”Ÿæˆ time_series
        if 'date' in merged.columns:
            ts_cols = [c for c in ['date', 'channel', 'orders', 'revenue', 'aov', 'cancel_rate', 'prep_time'] if c in merged.columns]
            operational_data["time_series"] = merged[ts_cols].to_dict('records')
            
        # è´¨é‡æŠ¥å‘Š
        needed = ['date', 'orders', 'revenue']
        operational_data["data_quality_report"]["missing_fields"] = [f for f in needed if f not in merged.columns]
        
        return operational_data

    @staticmethod
    def clean_numeric(val):
        if pd.isna(val): return 0
        if isinstance(val, (int, float)): return val
        clean_val = re.sub(r'[^\d\.]', '', str(val))
        try: return float(clean_val)
        except: return 0

# ============================
# CONFIG
# ============================
def get_secret(key):
    # ä¼˜å…ˆå°è¯•ä» Streamlit Secrets è¯»å–
    try:
        return st.secrets[key]
    except (FileNotFoundError, KeyError):
        # å›é€€åˆ°ç¯å¢ƒå˜é‡
        return os.getenv(key)

OPENAI_API_KEY = get_secret("OPENAI_API_KEY")
GOOGLE_API_KEY = get_secret("GOOGLE_MAPS_API_KEY")
YELP_API_KEY = get_secret("YELP_API_KEY")
CENSUS_API_KEY = get_secret("CENSUS_API_KEY")

PDF_STYLE_FILES = [
    "data/Aurainsighté—¨åº—åˆ†æã€ä¸œå—é£ç¾é£Ÿã€‘.txt",
    "data/æ ·æœ¬3.txt"
]



try:
    from meteostat import Point, Daily
except Exception as e:
    import streamlit as st
    st.error(f"Missing dependency: meteostat. Please check requirements.txt. Error: {e}")
    st.stop()
# ============================
# PDF STYLE LOADER
# ============================
def load_pdf_text(path):
    if not os.path.exists(path):
        st.warning(f"Warning: Style file not found: {path}. Skipping.")
        return ""
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()
    return text

STYLE_CONTEXT = "\n".join([load_pdf_text(p) for p in PDF_STYLE_FILES])

# ============================
# GOOGLE PLACE
# ============================
def google_search(query):
    url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
    params = {"query": query, "key": GOOGLE_API_KEY}
    try:
        response = requests.get(url, params=params)
        data = response.json()
        if data.get("status") == "OK":
            return data["results"]
        else:
            # è¿”å›é”™è¯¯çŠ¶æ€ä»¥ä¾¿è°ƒè¯•
            return [{"error": f"Google API Error: {data.get('status')} - {data.get('error_message', '')}"}]
    except Exception as e:
        return [{"error": f"Request failed: {str(e)}"}]

def get_google_reviews(place_id):
    if not GOOGLE_API_KEY:
        return []
    url = "https://maps.googleapis.com/maps/api/place/details/json"
    # æˆ‘ä»¬åªéœ€è¦ reviews å­—æ®µ
    params = {
        "place_id": place_id,
        "fields": "reviews",
        "key": GOOGLE_API_KEY,
        "language": "zh-CN" # å°è¯•è·å–ä¸­æ–‡è¯„è®ºï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚ä¸åŠ æ­¤å‚æ•°è·å–åŸè¯­è¨€
    }
    try:
        response = requests.get(url, params=params)
        data = response.json()
        if data.get("status") == "OK":
            return data.get("result", {}).get("reviews", [])
        return []
    except Exception:
        return []

def get_google_photo_url(photo_ref, max_width=400):
    if not photo_ref:
        return None
    base_url = "https://maps.googleapis.com/maps/api/place/photo"
    return f"{base_url}?maxwidth={max_width}&photo_reference={photo_ref}&key={GOOGLE_API_KEY}"

# ============================
# YELP
# ============================
def yelp_match(name, lat, lng):
    if not YELP_API_KEY:
        st.error("Yelp API Key is missing.")
        return []
    
    url = "https://api.yelp.com/v3/businesses/search"
    headers = {"Authorization": f"Bearer {YELP_API_KEY}"}
    params = {"term": name, "latitude": lat, "longitude": lng, "limit": 3}
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            return response.json().get("businesses", [])
        else:
            st.warning(f"Yelp API returned status: {response.status_code}")
            return []
    except Exception as e:
        st.warning(f"Yelp API call failed: {str(e)}")
        return []

def get_yelp_reviews(business_id):
    if not YELP_API_KEY:
        return []
    
    url = f"https://api.yelp.com/v3/businesses/{business_id}/reviews"
    headers = {"Authorization": f"Bearer {YELP_API_KEY}"}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json().get("reviews", [])
        return []
    except Exception:
        return []

def get_yelp_details(business_id):
    if not YELP_API_KEY:
        return {}
    
    url = f"https://api.yelp.com/v3/businesses/{business_id}"
    headers = {"Authorization": f"Bearer {YELP_API_KEY}"}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json()
        return {}
    except Exception:
        return {}

def analyze_sentiment(reviews):
    if not reviews:
        return {"score": 0, "label": "No Data", "keywords": []}
    
    full_text = " ".join([r.get("text", "") for r in reviews])
    blob = TextBlob(full_text)
    sentiment_score = blob.sentiment.polarity
    
    # Simple labeling
    if sentiment_score > 0.3:
        label = "Positive ğŸ˜Š"
    elif sentiment_score < -0.1:
        label = "Negative ğŸ˜"
    else:
        label = "Neutral ğŸ˜"
        
    # Extract keywords (simple noun phrases)
    keywords = list(set([w.lower() for w in blob.noun_phrases if len(w) > 3]))[:5]
    
    return {
        "score": sentiment_score,
        "label": label,
        "keywords": keywords
    }


# ============================
# METEOSTAT
# ============================
def get_weather(lat, lng, days=30):
    try:
        loc = Point(lat, lng)
        end = datetime.now()
        start = end - timedelta(days=days)
        df = Daily(loc, start, end).fetch()
        return df.reset_index()
    except Exception as e:
        st.warning(f"Weather data fetch failed: {e}")
        return pd.DataFrame()

# ============================
# NOAA CURRENT
# ============================
def noaa_forecast(lat, lng):
    try:
        url = f"https://api.weather.gov/points/{lat},{lng}"
        # NOAA requires User-Agent
        headers = {"User-Agent": "AuraInsight-App/1.0"}
        meta_resp = requests.get(url, headers=headers)
        if meta_resp.status_code != 200:
            return {}
        
        meta = meta_resp.json()
        forecast_url = meta.get("properties", {}).get("forecast")
        if not forecast_url:
            return {}
            
        forecast_resp = requests.get(forecast_url, headers=headers)
        return forecast_resp.json() if forecast_resp.status_code == 200 else {}
    except Exception:
        return {}

# ============================
# CENSUS
# ============================
def census_data(lat, lng):
    # simplified demo placeholder
    return {
        "population_est": "40,000â€“60,000",
        "asian_ratio": "40%â€“55%",
        "median_income": "$90kâ€“$110k"
    }

# ============================
# PDF EXPORT
# ============================
def export_pdf(text, filename):
    c = canvas.Canvas(filename, pagesize=letter)
    width, height = letter
    y = height - 40
    # Simple text wrapping logic
    for paragraph in text.split("\n"):
        # Split long lines roughly
        while len(paragraph) > 0:
            line = paragraph[:90] # Approx chars per line
            paragraph = paragraph[90:]
            if y < 40:
                c.showPage()
                y = height - 40
            # Register a font that supports utf-8 if needed, but for now standard
            c.drawString(40, y, line)
            y -= 14
    c.save()

# ... (ä¸­é—´ä»£ç ä¿æŒä¸å˜) ...

# ============================
# AI REPORT ENGINE
# ============================
def json_serial(obj):
    """JSON serializer for objects not serializable by default json code"""
    if isinstance(obj, (datetime, datetime.date)):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def generate_report(data, lang="zh", operational_data=None):
    client = OpenAI(api_key=OPENAI_API_KEY)

    # å‡†å¤‡å˜é‡
    restaurant_name = data.get("place", {}).get("name", "Unknown Restaurant")
    restaurant_address = data.get("place", {}).get("formatted_address", "Unknown Address")
    
    # æ„å»º Payload (Canonical Schema v2.1)
    payload = {
        "restaurant_profile": data.get("place"),
        "weather": {"history": data.get("weather_history"), "forecast": data.get("noaa_forecast")},
        "operational_data": operational_data if operational_data else "MISSING - USE INDUSTRY PRIORS",
        "timestamp": datetime.now().isoformat()
    }
    
    input_data_str = json.dumps(payload, ensure_ascii=False, indent=2, default=json_serial)
    
    # å¼ºåˆ¶é‡åŒ–æŒ‡ä»¤ (v2.1)
    system_instruction = f"""
    You are AuraInsight v2.1 (Quantitative Mode). 
    
    MANDATORY STANDARDS:
    1. If 'operational_data' is present, you MUST use [VERIFIED] data for all core KPIs.
    2. Every forecast (Orders/Revenue) MUST include a P10, P50, and P90 confidence interval.
    3. You MUST provide the specific formula used for your prediction (e.g., Bayesian Linear Regression with Weather Coefficients).
    4. If weather history is present, calculate the correlation coefficient between rain/temp and order volume.
    5. Output Language: {"Chinese" if lang == "zh" else "English"}.
    """
    
    input_payload = input_data_str + f"\n\n[SYSTEM_DIRECTIVE]: {system_instruction}"

    try:
        # æ¨¡å‹è·¯ç”±é€»è¾‘
        if AURAINSIGHT_MODEL_MODE == "explicit_model":
            # æ¨¡å¼ Bï¼šæ˜¾å¼å¼ºæ§
            response = client.responses.create(
                model=AURAINSIGHT_MODEL_ID,
                prompt={
                    "variables": {
                        "restaurant_name": restaurant_name,
                        "restaurant_address": restaurant_address,
                        "input_data": input_payload
                    }
                }
            )
            # è®°å½•æ¨¡å‹æ—¥å¿—
            st.info(f"Using Explicit Model: {AURAINSIGHT_MODEL_ID}")
        else:
            # æ¨¡å¼ Aï¼šPrompt ç»‘å®š (é»˜è®¤)
            response = client.responses.create(
                prompt={
                    "id": AURAINSIGHT_PROMPT_ID,
                    "version": "2",
                    "variables": {
                        "restaurant_name": restaurant_name,
                        "restaurant_address": restaurant_address,
                        "input_data": input_payload
                    }
                }
            )
            st.info("Using Prompt-Bound Model Mapping")

        
        # 2. è½®è¯¢çŠ¶æ€ï¼Œç›´åˆ°å®Œæˆ (OpenAI Responses API æ˜¯å¼‚æ­¥çš„)
        import time
        max_retries = 30 # æœ€å¤šç­‰å¾… 60 ç§’ (2s * 30)
        retries = 0
        final_response = response
        
        while retries < max_retries:
            # æ£€æŸ¥å½“å‰çŠ¶æ€
            # å¦‚æœçŠ¶æ€å·²ç»æ˜¯ completed æˆ– failedï¼Œé€€å‡ºå¾ªç¯
            if hasattr(final_response, 'status'):
                if final_response.status == 'completed':
                    break
                if final_response.status in ['failed', 'incomplete', 'cancelled']:
                    return f"âŒ AI å“åº”å¤±è´¥ (çŠ¶æ€: {final_response.status})"
            
            # ç­‰å¾…å¹¶é‡æ–°è·å–çŠ¶æ€
            time.sleep(2)
            final_response = client.responses.retrieve(final_response.id)
            retries += 1
            
        # 3. è§£ææœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        text_content = ""
        if hasattr(final_response, 'output') and isinstance(final_response.output, list):
            for item in final_response.output:
                # å¯»æ‰¾ç±»å‹ä¸º message çš„è¾“å‡ºé¡¹
                if hasattr(item, 'content') and isinstance(item.content, list):
                    for part in item.content:
                        # å¤„ç†æ–‡æœ¬å—
                        if hasattr(part, 'text'):
                            # æœ‰äº›ç‰ˆæœ¬æ˜¯ part.text.valueï¼Œæœ‰äº›æ˜¯ part.text
                            if hasattr(part.text, 'value'):
                                text_content += part.text.value
                            elif isinstance(part.text, str):
                                text_content += part.text
                        elif isinstance(part, dict) and 'text' in part:
                            t = part['text']
                            text_content += t.get('value', t) if isinstance(t, dict) else str(t)
        
        if text_content:
            return text_content
            
        # å…œåº•æ˜¾ç¤ºï¼šå¦‚æœè½®è¯¢è¶…æ—¶æˆ–è§£æå¤±è´¥
        if hasattr(final_response, 'model_dump_json'):
            return f"âš ï¸ æŠ¥å‘Šç”Ÿæˆè¶…æ—¶æˆ–è§£æå¤±è´¥ã€‚åŸå§‹ JSONï¼š\n\n{final_response.model_dump_json(indent=2, ensure_ascii=False)}"
        
        return f"âš ï¸ æ— æ³•è·å–æŠ¥å‘Šå†…å®¹ã€‚çŠ¶æ€: {getattr(final_response, 'status', 'unknown')}"

    except AttributeError as ae:
        return f"âŒ OpenAI åº“ç‰ˆæœ¬ä¸æ”¯æŒæ­¤æ“ä½œæˆ– API ç»“æ„å·²å˜æ›´: {str(ae)}"
    except Exception as e:
        return f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

# ... (ä¸­é—´ä»£ç ä¿æŒä¸å˜) ...

# ============================
# STREAMLIT UI
# ============================
st.set_page_config(page_title="AuraInsight v2.1", layout="wide")

# Sidebar Monitoring
with st.sidebar:
    st.image("https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/restaurant-71.png", width=50)
    st.title("System Status")
    st.caption(f"Model Mode: {AURAINSIGHT_MODEL_MODE}")
    st.caption(f"Pipeline: {'Enabled' if ENABLE_DATA_UPLOAD_PIPELINE else 'Disabled'}")
    if st.toggle("Debug Info"):
        st.json({"model_id": AURAINSIGHT_MODEL_ID, "prompt_id": AURAINSIGHT_PROMPT_ID})

st.title("AuraInsight Â· é‡åŒ–å¢é•¿åˆ†æç³»ç»Ÿ v2.1")

# 1. æœç´¢ä¸é€‰æ‹©
address_input = st.text_input("è¯·è¾“å…¥é¤å…åœ°å€", placeholder="ä¾‹å¦‚ï¼š2406 19th Ave, San Francisco")

if address_input:
    # æœç´¢é€»è¾‘
    if "last_query" not in st.session_state or st.session_state.last_query != address_input:
        # è‡ªåŠ¨è¿½åŠ  "restaurant" ä»¥ç¡®ä¿æœç´¢çš„æ˜¯å•†å®¶è€Œä¸æ˜¯çº¯åœ°å€
        search_query = f"{address_input} restaurant"
        st.session_state.search_results = google_search(search_query)
        st.session_state.last_query = address_input
    
    results = st.session_state.get("search_results", [])

    # é”™è¯¯å¤„ç†é€»è¾‘
    if not results:
        st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„é¤å…ï¼Œè¯·å°è¯•æ›´è¯¦ç»†çš„åœ°å€ã€‚")
    elif isinstance(results[0], dict) and "error" in results[0]:
        st.error(results[0]["error"])
    else:
        options = [f"{r['name']} | {r['formatted_address']}" for r in results]
        # ä½¿ç”¨ key ä¿æŒ selectbox çŠ¶æ€
        idx = st.selectbox("è¯·ç¡®è®¤åŒ¹é…çš„å•†å®¶", range(len(options)), format_func=lambda i: options[i], key="selected_idx")
        
        if idx is not None:
            place = results[idx]
            
            # 2. ç¡®è®¤æŒ‰é’®ä¸åŠ¨æ€è¿›åº¦æ¡
            if st.button("ğŸš€ ç¡®è®¤å¹¶å¼€å§‹åˆ†æå•†å®¶æ•°æ®"):
                progress_bar = st.progress(0, text="æ­£åœ¨åˆå§‹åŒ–åˆ†æ...")
                
                try:
                    lat = place["geometry"]["location"]["lat"]
                    lng = place["geometry"]["location"]["lng"]
                    
                    # æ­¥éª¤ 1: Yelp & Google è¯„è®º
                    progress_bar.progress(25, text="æ­£åœ¨è·å– Yelp ä¸ Google è¯„è®ºæ•°æ®...")
                    yelp_data = yelp_match(place["name"], lat, lng)
                    
                    # 1.1: è·å–è¯„è®ºä¸æƒ…æ„Ÿåˆ†æ
                    yelp_reviews = []
                    google_reviews = []
                    
                    # è·å– Google è¯„è®º
                    try:
                        google_reviews = get_google_reviews(place["place_id"])
                    except Exception:
                        pass

                    # è·å– Yelp è¯„è®ºä¸è¯¦æƒ…
                    yelp_details = {}
                    if yelp_data:
                        try:
                            first_biz_id = yelp_data[0]['id']
                            yelp_reviews = get_yelp_reviews(first_biz_id)
                            # è·å–å•†å®¶è¯¦æƒ…ä»¥æ‹‰å–æ›´å¤šå›¾ç‰‡
                            yelp_details = get_yelp_details(first_biz_id)
                        except Exception:
                            pass
                    
                    # åˆå¹¶è¯„è®ºè¿›è¡Œåˆ†æ
                    # æ³¨æ„ï¼šGoogle è¯„è®ºå¯¹è±¡ä¹Ÿæœ‰ 'text' å­—æ®µï¼Œä¸ Yelp ç»“æ„å…¼å®¹
                    all_reviews = google_reviews + yelp_reviews
                    sentiment_result = analyze_sentiment(all_reviews)
                    
                    # æ­¥éª¤ 2: å¤©æ°”
                    progress_bar.progress(50, text="æ­£åœ¨è·å–å†å²ä¸é¢„æµ‹å¤©æ°”æ•°æ®...")
                    weather_hist = get_weather(lat, lng)
                    noaa = noaa_forecast(lat, lng)
                    
                    # æ­¥éª¤ 3: äººå£æ™®æŸ¥
                    progress_bar.progress(75, text="æ­£åœ¨æŸ¥è¯¢å•†åœˆäººå£æ™®æŸ¥æ•°æ®...")
                    census = census_data(lat, lng)
                    
                    # å®Œæˆ
                    st.session_state.fetched_data = {
                        "place": place,
                        "yelp": yelp_data,
                        "yelp_details": yelp_details,
                        "yelp_reviews": yelp_reviews,
                        "google_reviews": google_reviews,
                        "sentiment": sentiment_result,
                        "weather_history": weather_hist.tail(10).to_dict(),
                        "noaa_forecast": noaa,
                        "census": census
                    }
                    st.session_state.current_place_id = place["place_id"]
                    
                    # æ¸…é™¤æ—§çš„æ·±åº¦æŠ¥å‘Š
                    if "report_content" in st.session_state:
                        del st.session_state.report_content
                        
                    progress_bar.progress(100, text="æ•°æ®æ‹‰å–å®Œæˆï¼")
                    
                except Exception as e:
                    st.error(f"æ•°æ®æ‹‰å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                    progress_bar.empty()

            # 3. æ˜¾ç¤ºå•†å®¶æ¦‚è¦ (ä»…å½“æ•°æ®å·²æ‹‰å–æ—¶æ˜¾ç¤º)
            if "fetched_data" in st.session_state and st.session_state.current_place_id == place["place_id"]:
                data = st.session_state.fetched_data
                
                st.divider()
                st.subheader("ğŸ“Š å•†å®¶æ•°æ®æ¦‚è¦")
                
                # Photos Gallery
                all_photo_urls = []
                
                # Google Photos (Up to 6)
                if "photos" in place:
                    for photo in place["photos"][:6]:
                        url = get_google_photo_url(photo.get("photo_reference"), max_width=800)
                        if url:
                            all_photo_urls.append(("Google", url))
                
                # Yelp Photos
                if data.get("yelp_details") and "photos" in data["yelp_details"]:
                    for url in data["yelp_details"]["photos"]:
                        all_photo_urls.append(("Yelp", url))
                
                if all_photo_urls:
                    st.markdown("#### ğŸ“¸ é—¨åº—å®æ™¯ä¸èœå“é¢„è§ˆ")
                    # ä½¿ç”¨ 3 åˆ—ç½‘æ ¼å±•ç¤ºå›¾ç‰‡
                    cols = st.columns(3)
                    for i, (source, url) in enumerate(all_photo_urls):
                        with cols[i % 3]:
                            st.image(url, caption=f"æ¥æº: {source}", use_column_width=True)
                                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.info(f"**Google è¯„åˆ†**: {place.get('rating', 'N/A')} ({place.get('user_ratings_total', 0)} æ¡)")
                with col2:
                    yelp_rating = data['yelp'][0]['rating'] if data['yelp'] else "N/A"
                    yelp_count = data['yelp'][0]['review_count'] if data['yelp'] else 0
                    st.error(f"**Yelp è¯„åˆ†**: {yelp_rating} ({yelp_count} æ¡)")
                with col3:
                    st.success(f"**å•†åœˆäººå£ (3è‹±é‡Œ)**: {data['census']['population_est']}")

                # Sentiment
                if data.get("sentiment"):
                    st.markdown("#### ğŸ’¬ è¯„è®ºæƒ…æ„Ÿæ´å¯Ÿ")
                    sent = data["sentiment"]
                    s_col1, s_col2 = st.columns([1, 2])
                    with s_col1:
                        st.metric("æƒ…æ„Ÿå€¾å‘", sent.get("label", "N/A"), f"{sent.get('score', 0):.2f}")
                    with s_col2:
                        if sent.get("keywords"):
                            st.write("**çƒ­é—¨å…³é”®è¯:**")
                            st.write(" ".join([f"`{k}`" for k in sent["keywords"]]))
                        else:
                            st.write("æš‚æ— è¶³å¤Ÿè¯„è®ºæå–å…³é”®è¯")

                with st.expander("æŸ¥çœ‹è¯¦ç»†åŸå§‹æ•°æ®"):
                    st.json(data)

                st.divider()

                # 4. æ·±åº¦åˆ†ææŒ‰é’®
                col_btn, col_lang = st.columns([1, 1])
                with col_lang:
                    lang = st.selectbox("æŠ¥å‘Šè¯­è¨€", ["zh", "en"], key="report_lang")
                
                with col_btn:
                    if st.button("ğŸ” ç”Ÿæˆæ·±åº¦AIç­–ç•¥æŠ¥å‘Š", type="primary"):
                        # åˆå§‹åŒ–è¿›åº¦æ¡
                        report_progress = st.progress(0, text="æ­£åœ¨å¯åŠ¨ AI å¼•æ“...")
                        
                        try:
                            # é˜¶æ®µ 1: å‡†å¤‡ä¸Šä¸‹æ–‡
                            report_progress.progress(10, text="æ­£åœ¨æ•´åˆå•†å®¶æ•°æ®ä¸å•†åœˆä¿¡æ¯...")
                            time.sleep(0.5)
                            
                            # é˜¶æ®µ 2: æ„å»º Prompt
                            report_progress.progress(30, text="æ­£åœ¨æ„å»ºé«˜ç»´åˆ†ææ¨¡å‹...")
                            
                            # é˜¶æ®µ 3: å¼‚æ­¥è°ƒç”¨ API å¹¶åŠ¨æ€æ›´æ–°æ–‡å­—
                            loading_texts = [
                                "æ­£åœ¨é€šè¿‡ GPT-4o è¿›è¡Œæ·±åº¦æ¨ç†...",
                                "æ­£åœ¨åˆ†æ Yelp ä¸ Google è¯„è®ºæƒ…æ„Ÿè¶‹åŠ¿...",
                                "æ­£åœ¨ç»“åˆ Meteostat å†å²å¤©æ°”æ•°æ®è¿›è¡Œå›å½’åˆ†æ...",
                                "æ­£åœ¨äº¤å‰æ¯”å¯¹ Census å•†åœˆäººå£ç»Ÿè®¡æ•°æ®...",
                                "æ­£åœ¨ç”Ÿæˆéº¦è‚¯é”¡é£æ ¼çš„æˆ˜ç•¥å»ºè®®...",
                                "æ­£åœ¨ä¼˜åŒ–æŠ¥å‘Šæ ¼å¼ä¸æ’ç‰ˆ..."
                            ]
                            
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(generate_report, data, lang)
                                
                                # å¾ªç¯æ›´æ–°è¿›åº¦æ¡æ–‡å­—ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆ
                                idx = 0
                                progress_val = 30
                                while not future.done():
                                    if idx < len(loading_texts):
                                        current_text = loading_texts[idx]
                                    else:
                                        # å¦‚æœæ‰€æœ‰é¢„è®¾æ–‡æ¡ˆéƒ½æ˜¾ç¤ºå®Œäº†ï¼Œä¸å†å¾ªç¯ï¼Œè€Œæ˜¯æ˜¾ç¤ºé€šç”¨ç­‰å¾…æç¤º
                                        current_text = "æ­£åœ¨è¿›è¡Œæœ€ç»ˆçš„æ·±åº¦é€»è¾‘æ•´åˆï¼Œè¯·è€å¿ƒç­‰å¾…..."
                                    
                                    # è®©è¿›åº¦æ¡ç¼“æ…¢å¢åŠ ï¼Œä½†ä¸åˆ° 100%
                                    if progress_val < 90:
                                        progress_val += 1
                                    
                                    report_progress.progress(progress_val, text=f"AI é¡¾é—®å·¥ä½œæµ: {current_text}")
                                    time.sleep(1.5) # æ¯ 1.5 ç§’åˆ‡æ¢ä¸€æ¬¡æ–‡å­—
                                    idx += 1
                                
                                # è·å–ç»“æœ
                                report = future.result()
                            
                            # é˜¶æ®µ 4: å¤„ç†å“åº”
                            report_progress.progress(95, text="æ­£åœ¨æœ€ç»ˆæ ¼å¼åŒ–æŠ¥å‘Šå†…å®¹...")
                            st.session_state.report_content = report
                            
                            # å®Œæˆ
                            report_progress.progress(100, text="æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼")
                            
                        except Exception as e:
                            report_progress.empty()
                            st.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            
            # 5. å¯ç¼–è¾‘æŠ¥å‘Šä¸å¯¼å‡º
            if "report_content" in st.session_state and st.session_state.current_place_id == place["place_id"]:
                st.divider()
                st.subheader("ğŸ“ æ·±åº¦åˆ†ææŠ¥å‘Š (å¯ç¼–è¾‘)")
                
                # ç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æŠ¥å‘Š
                user_edited_report = st.text_area(
                    "æ‚¨å¯ä»¥ç›´æ¥ä¿®æ”¹ä¸‹æ–¹çš„æŠ¥å‘Šå†…å®¹ï¼Œä¿®æ”¹åç‚¹å‡»ä¸‹è½½å³å¯ã€‚",
                    value=st.session_state.report_content,
                    height=500,
                    key="report_area"
                )
                
                # å¯¼å‡ºæŒ‰é’®
                col_exp1, col_exp2 = st.columns([1, 1])
                with col_exp1:
                    if st.button("ğŸ“¥ å¯¼å‡º PDF åˆ†ææŠ¥å‘Š"):
                        export_pdf(user_edited_report, "analysis_report.pdf")
                        with open("analysis_report.pdf", "rb") as pdf_file:
                            st.download_button(
                                label="ç‚¹å‡»ä¸‹è½½ PDF",
                                data=pdf_file,
                                file_name="AuraInsight_Report.pdf",
                                mime="application/pdf"
                            )
                        st.success("PDF å·²ç”Ÿæˆï¼")

                # ============================
                # é˜¶æ®µ 1.2: è¡¥å……æ•°æ®ä¸Šä¼ åŒºåŸŸ (é—­ç¯æ ¸å¿ƒ)
                # ============================
                if ENABLE_DATA_UPLOAD_PIPELINE:
                    st.divider()
                    st.markdown("### ğŸ“Š è¡¥å……è¿è¥æ•°æ®ï¼ˆæ•°æ®é—­ç¯ï¼‰")
                    st.info("ğŸ’¡ ä¸Šä¼ çœŸå®æ•°æ®ï¼ˆPOS/å¤–å–å¹³å°å¯¼å‡ºï¼‰åï¼ŒAI å°†é‡æ–°æ¸…æ´—å¹¶æ ¡å‡†æ¨¡å‹ç»“è®ºï¼Œæä¾›æ›´é«˜ç²¾åº¦çš„æŠ¥å‘Šã€‚")
                    
                    uploaded_files = st.file_uploader(
                        "æ”¯æŒ CSV, XLSX, TXT (æ”¯æŒå¤šæ–‡ä»¶åŒæ—¶ä¸Šä¼ )", 
                        accept_multiple_files=True,
                        type=['csv', 'xlsx', 'xls', 'txt']
                    )
                    
                    if uploaded_files:
                        op_data = DataPipeline.build_canonical_schema(uploaded_files)
                        
                        # å¯è§†åŒ–åé¦ˆ
                        v_col1, v_col2 = st.columns([2, 1])
                        with v_col1:
                            st.write("**æ•°æ®è´¨é‡å®¡è®¡**")
                            if op_data["data_quality_report"]["missing_fields"]:
                                st.warning(f"ç¼ºå¤±å…³é”®å­—æ®µ: {', '.join(op_data['data_quality_report']['missing_fields'])}")
                            if op_data["time_series"]:
                                st.success(f"è§£ææˆåŠŸ: è·å–åˆ° {len(op_data['time_series'])} æ¡è®°å½•")
                                st.dataframe(pd.DataFrame(op_data["time_series"]).head(10))
                        
                        with v_col2:
                            st.write("**å­—æ®µæ¥æºè¿½æº¯**")
                            for item in op_data["traceability"]:
                                st.caption(f"{item['field']}: {item['tag']} ({item['source']})")
                        
                        # é‡æ–°ç”ŸæˆæŒ‰é’®
                        if st.button("ğŸ”„ æ³¨å…¥çœŸå®æ•°æ®å¹¶é‡æ–°å»ºæ¨¡ç”ŸæˆæŠ¥å‘Š", type="primary"):
                            with st.progress(0, text="æ­£åœ¨åŒæ­¥é‡åŒ–æ¨¡å‹..."):
                                new_report = generate_report(data, lang, operational_data=op_data)
                                st.session_state.report_content = new_report
                                st.rerun()


                    # Admin å›æ»šå¼€å…³ (éšè—)
                    if st.toggle("Admin: ä½¿ç”¨æ—§æ¨¡å‹ç‰ˆæœ¬ (Rollback Mode)", value=False):
                        st.session_state.use_legacy_model = True
                    else:
                        st.session_state.use_legacy_model = False

