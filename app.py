import os
import json
import math
import pandas as pd
import streamlit as st
from datetime import datetime, timedelta
import requests
import pandas as pd
import nltk
import time
import concurrent.futures
import io

# Ensure TextBlob corpora are downloaded
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')
try:
    nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
    nltk.download('averaged_perceptron_tagger')
try:
    nltk.data.find('corpora/brown')
except LookupError:
    nltk.download('brown')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

from meteostat import Point, Daily
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from openai import OpenAI
from textblob import TextBlob
import concurrent.futures
import re
import io

# ============================
# FEATURE FLAGS & CONFIG
# ============================
ENABLE_DATA_UPLOAD_PIPELINE = os.getenv("ENABLE_DATA_UPLOAD_PIPELINE", "true").lower() == "true"
AURAINSIGHT_MODEL = os.getenv("AURAINSIGHT_MODEL", "gpt-4o")

# ============================
# DATA PIPELINE (CANONICAL SCHEMA)
# ============================
class DataPipeline:
    COLUMN_MAP = {
        'æ—¥æœŸ': 'date', 'date': 'date', 'æ—¶é—´': 'date',
        'è®¢å•é‡': 'orders', 'å•é‡': 'orders', 'orders': 'orders', 'order_count': 'orders',
        'è¥æ”¶': 'revenue', 'å®æ”¶': 'revenue', 'revenue': 'revenue', 'sales': 'revenue', 'é‡‘é¢': 'revenue',
        'å®¢å•ä»·': 'aov', 'aov': 'aov', 'å¹³å‡å•ä»·': 'aov',
        'å–æ¶ˆç‡': 'cancel_rate', 'é€€å•ç‡': 'cancel_rate', 'cancel_rate': 'cancel_rate',
        'å¤‡é¤æ—¶é—´': 'prep_time', 'prep_time': 'prep_time',
        'æ¸ é“': 'channel', 'æ¥æº': 'channel', 'channel': 'channel'
    }

    @staticmethod
    def clean_numeric(val):
        if pd.isna(val): return 0
        if isinstance(val, (int, float)): return val
        # å»é™¤è´§å¸ç¬¦å·ã€åƒåˆ†ä½ã€ç™¾åˆ†å·
        clean_val = re.sub(r'[^\d\.]', '', str(val))
        try:
            return float(clean_val)
        except:
            return 0

    @classmethod
    def parse_file(cls, uploaded_file):
        fname = uploaded_file.name
        ext = fname.split('.')[-1].lower()
        df = pd.DataFrame()
        
        try:
            if ext in ['csv']:
                df = pd.read_csv(uploaded_file)
            elif ext in ['xlsx', 'xls']:
                df = pd.read_excel(uploaded_file)
            elif ext in ['txt']:
                content = uploaded_file.read().decode("utf-8")
                return {"type": "text", "content": content, "source": fname}
            else:
                return {"error": f"æš‚ä¸æ”¯æŒæ ¼å¼: {ext}"}
            
            # åŸºç¡€æ¸…æ´—ï¼šæ˜ å°„åˆ—å
            df = df.rename(columns=lambda x: cls.COLUMN_MAP.get(str(x).lower().strip(), x))
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality = {
                "missing_cols": [c for c in ['date', 'orders', 'revenue'] if c not in df.columns],
                "rows": len(df),
                "source": fname
            }
            
            return {"type": "table", "data": df, "quality": quality, "source": fname}
        except Exception as e:
            return {"error": f"è§£æå¤±è´¥ ({fname}): {str(e)}"}

    @classmethod
    def process_bundle(cls, files):
        bundle = {"verified": {}, "derived": {}, "assumed": {}, "traceability": []}
        all_dfs = []
        
        for f in files:
            res = cls.parse_file(f)
            if "error" in res:
                st.error(res["error"])
                continue
            if res["type"] == "table":
                df = res["data"]
                all_dfs.append(df)
                for col in df.columns:
                    if col in cls.COLUMN_MAP.values():
                        bundle["verified"][col] = True
                        bundle["traceability"].append({"field": col, "source": res["source"], "tag": "VERIFIED_DATA"})

        # æ¨å¯¼æ•°æ®
        if "revenue" in bundle["verified"] and "orders" in bundle["verified"]:
            bundle["derived"]["aov"] = True
            bundle["traceability"].append({"field": "aov", "source": "Logic Calculation", "tag": "DERIVED_DATA"})
            
        return bundle, all_dfs

# ============================
# CONFIG
# ============================
def get_secret(key):
    # ä¼˜å…ˆå°è¯•ä» Streamlit Secrets è¯»å–
    try:
        return st.secrets[key]
    except (FileNotFoundError, KeyError):
        # å›é€€åˆ°ç¯å¢ƒå˜é‡
        return os.getenv(key)

OPENAI_API_KEY = get_secret("OPENAI_API_KEY")
GOOGLE_API_KEY = get_secret("GOOGLE_MAPS_API_KEY")
YELP_API_KEY = get_secret("YELP_API_KEY")
CENSUS_API_KEY = get_secret("CENSUS_API_KEY")

PDF_STYLE_FILES = [
    "data/Aurainsighté—¨åº—åˆ†æã€ä¸œå—é£ç¾é£Ÿã€‘.txt",
    "data/æ ·æœ¬3.txt"
]



try:
    from meteostat import Point, Daily
except Exception as e:
    import streamlit as st
    st.error(f"Missing dependency: meteostat. Please check requirements.txt. Error: {e}")
    st.stop()
# ============================
# PDF STYLE LOADER
# ============================
def load_pdf_text(path):
    if not os.path.exists(path):
        st.warning(f"Warning: Style file not found: {path}. Skipping.")
        return ""
    with open(path, "r", encoding="utf-8") as f:
        text = f.read()
    return text

STYLE_CONTEXT = "\n".join([load_pdf_text(p) for p in PDF_STYLE_FILES])

# ============================
# GOOGLE PLACE
# ============================
def google_search(query):
    url = "https://maps.googleapis.com/maps/api/place/textsearch/json"
    params = {"query": query, "key": GOOGLE_API_KEY}
    try:
        response = requests.get(url, params=params)
        data = response.json()
        if data.get("status") == "OK":
            return data["results"]
        else:
            # è¿”å›é”™è¯¯çŠ¶æ€ä»¥ä¾¿è°ƒè¯•
            return [{"error": f"Google API Error: {data.get('status')} - {data.get('error_message', '')}"}]
    except Exception as e:
        return [{"error": f"Request failed: {str(e)}"}]

def get_google_reviews(place_id):
    if not GOOGLE_API_KEY:
        return []
    url = "https://maps.googleapis.com/maps/api/place/details/json"
    # æˆ‘ä»¬åªéœ€è¦ reviews å­—æ®µ
    params = {
        "place_id": place_id,
        "fields": "reviews",
        "key": GOOGLE_API_KEY,
        "language": "zh-CN" # å°è¯•è·å–ä¸­æ–‡è¯„è®ºï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚ä¸åŠ æ­¤å‚æ•°è·å–åŸè¯­è¨€
    }
    try:
        response = requests.get(url, params=params)
        data = response.json()
        if data.get("status") == "OK":
            return data.get("result", {}).get("reviews", [])
        return []
    except Exception:
        return []

def get_google_photo_url(photo_ref, max_width=400):
    if not photo_ref:
        return None
    base_url = "https://maps.googleapis.com/maps/api/place/photo"
    return f"{base_url}?maxwidth={max_width}&photo_reference={photo_ref}&key={GOOGLE_API_KEY}"

# ============================
# YELP
# ============================
def yelp_match(name, lat, lng):
    if not YELP_API_KEY:
        st.error("Yelp API Key is missing.")
        return []
    
    url = "https://api.yelp.com/v3/businesses/search"
    headers = {"Authorization": f"Bearer {YELP_API_KEY}"}
    params = {"term": name, "latitude": lat, "longitude": lng, "limit": 3}
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            return response.json().get("businesses", [])
        else:
            st.warning(f"Yelp API returned status: {response.status_code}")
            return []
    except Exception as e:
        st.warning(f"Yelp API call failed: {str(e)}")
        return []

def get_yelp_reviews(business_id):
    if not YELP_API_KEY:
        return []
    
    url = f"https://api.yelp.com/v3/businesses/{business_id}/reviews"
    headers = {"Authorization": f"Bearer {YELP_API_KEY}"}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json().get("reviews", [])
        return []
    except Exception:
        return []

def get_yelp_details(business_id):
    if not YELP_API_KEY:
        return {}
    
    url = f"https://api.yelp.com/v3/businesses/{business_id}"
    headers = {"Authorization": f"Bearer {YELP_API_KEY}"}
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json()
        return {}
    except Exception:
        return {}

def analyze_sentiment(reviews):
    if not reviews:
        return {"score": 0, "label": "No Data", "keywords": []}
    
    full_text = " ".join([r.get("text", "") for r in reviews])
    blob = TextBlob(full_text)
    sentiment_score = blob.sentiment.polarity
    
    # Simple labeling
    if sentiment_score > 0.3:
        label = "Positive ğŸ˜Š"
    elif sentiment_score < -0.1:
        label = "Negative ğŸ˜"
    else:
        label = "Neutral ğŸ˜"
        
    # Extract keywords (simple noun phrases)
    keywords = list(set([w.lower() for w in blob.noun_phrases if len(w) > 3]))[:5]
    
    return {
        "score": sentiment_score,
        "label": label,
        "keywords": keywords
    }


# ============================
# METEOSTAT
# ============================
def get_weather(lat, lng, days=30):
    try:
        loc = Point(lat, lng)
        end = datetime.now()
        start = end - timedelta(days=days)
        df = Daily(loc, start, end).fetch()
        return df.reset_index()
    except Exception as e:
        st.warning(f"Weather data fetch failed: {e}")
        return pd.DataFrame()

# ============================
# NOAA CURRENT
# ============================
def noaa_forecast(lat, lng):
    try:
        url = f"https://api.weather.gov/points/{lat},{lng}"
        # NOAA requires User-Agent
        headers = {"User-Agent": "AuraInsight-App/1.0"}
        meta_resp = requests.get(url, headers=headers)
        if meta_resp.status_code != 200:
            return {}
        
        meta = meta_resp.json()
        forecast_url = meta.get("properties", {}).get("forecast")
        if not forecast_url:
            return {}
            
        forecast_resp = requests.get(forecast_url, headers=headers)
        return forecast_resp.json() if forecast_resp.status_code == 200 else {}
    except Exception:
        return {}

# ============================
# CENSUS
# ============================
def census_data(lat, lng):
    # simplified demo placeholder
    return {
        "population_est": "40,000â€“60,000",
        "asian_ratio": "40%â€“55%",
        "median_income": "$90kâ€“$110k"
    }

# ============================
# PDF EXPORT
# ============================
def export_pdf(text, filename):
    c = canvas.Canvas(filename, pagesize=letter)
    width, height = letter
    y = height - 40
    # Simple text wrapping logic
    for paragraph in text.split("\n"):
        # Split long lines roughly
        while len(paragraph) > 0:
            line = paragraph[:90] # Approx chars per line
            paragraph = paragraph[90:]
            if y < 40:
                c.showPage()
                y = height - 40
            # Register a font that supports utf-8 if needed, but for now standard
            c.drawString(40, y, line)
            y -= 14
    c.save()

# ... (ä¸­é—´ä»£ç ä¿æŒä¸å˜) ...

# ============================
# AI REPORT ENGINE
# ============================
def json_serial(obj):
    """JSON serializer for objects not serializable by default json code"""
    if isinstance(obj, (datetime, datetime.date)):
        return obj.isoformat()
    raise TypeError(f"Type {type(obj)} not serializable")

def generate_report(data, lang="zh", operational_data=None):
    client = OpenAI(api_key=OPENAI_API_KEY)

    # å‡†å¤‡å˜é‡
    restaurant_name = data.get("place", {}).get("name", "Unknown Restaurant")
    restaurant_address = data.get("place", {}).get("formatted_address", "Unknown Address")
    
    # æ„å»º Payload
    payload = {
        "restaurant_profile": data.get("place"),
        "reviews": {
            "google": data.get("google_reviews"),
            "yelp": data.get("yelp_reviews"),
            "sentiment": data.get("sentiment")
        },
        "weather": {
            "history": data.get("weather_history"),
            "forecast": data.get("noaa_forecast")
        },
        "census": data.get("census"),
        "operational_data": operational_data if operational_data else "MISSING - USE INDUSTRY ASSUMPTIONS"
    }
    
    input_data_str = json.dumps(payload, ensure_ascii=False, indent=2, default=json_serial)
    
    # æ³¨å…¥å¼ºåˆ¶æ€§æŒ‡ä»¤ (Master Prompt v1.1 Logic)
    system_instruction = f"""
    You are AuraInsight v1.1 Master Engine. 
    Output Model: {AURAINSIGHT_MODEL}
    
    MANDATORY RULES:
    1. If 'operational_data' contains VERIFIED/DERIVED values, you MUST override all assumed priors.
    2. Every quantitative conclusion MUST be tagged with [VERIFIED], [DERIVED], or [ASSUMPTION].
    3. Include a 'Data Traceability Audit' table at the end of the report.
    4. Provide P10/P50/P90 for all forecasts.
    5. Language: {"Chinese" if lang == "zh" else "English"}.
    """
    
    input_data_with_lang = input_data_str + f"\n\n[SYSTEM_DIRECTIVE]: {system_instruction}"

    try:
        # ä½¿ç”¨ Prompt Template ID
        response = client.responses.create(
            prompt={
                "id": "pmpt_6971b3bd094081959997af7730098d45020d02ec1efab62b",
                "version": "2",
                "variables": {
                    "restaurant_name": restaurant_name,
                    "restaurant_address": restaurant_address,
                    "input_data": input_data_with_lang
                }
            }
        )
        
        # 2. è½®è¯¢çŠ¶æ€ï¼Œç›´åˆ°å®Œæˆ (OpenAI Responses API æ˜¯å¼‚æ­¥çš„)
        import time
        max_retries = 30 # æœ€å¤šç­‰å¾… 60 ç§’ (2s * 30)
        retries = 0
        final_response = response
        
        while retries < max_retries:
            # æ£€æŸ¥å½“å‰çŠ¶æ€
            # å¦‚æœçŠ¶æ€å·²ç»æ˜¯ completed æˆ– failedï¼Œé€€å‡ºå¾ªç¯
            if hasattr(final_response, 'status'):
                if final_response.status == 'completed':
                    break
                if final_response.status in ['failed', 'incomplete', 'cancelled']:
                    return f"âŒ AI å“åº”å¤±è´¥ (çŠ¶æ€: {final_response.status})"
            
            # ç­‰å¾…å¹¶é‡æ–°è·å–çŠ¶æ€
            time.sleep(2)
            final_response = client.responses.retrieve(final_response.id)
            retries += 1
            
        # 3. è§£ææœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
        text_content = ""
        if hasattr(final_response, 'output') and isinstance(final_response.output, list):
            for item in final_response.output:
                # å¯»æ‰¾ç±»å‹ä¸º message çš„è¾“å‡ºé¡¹
                if hasattr(item, 'content') and isinstance(item.content, list):
                    for part in item.content:
                        # å¤„ç†æ–‡æœ¬å—
                        if hasattr(part, 'text'):
                            # æœ‰äº›ç‰ˆæœ¬æ˜¯ part.text.valueï¼Œæœ‰äº›æ˜¯ part.text
                            if hasattr(part.text, 'value'):
                                text_content += part.text.value
                            elif isinstance(part.text, str):
                                text_content += part.text
                        elif isinstance(part, dict) and 'text' in part:
                            t = part['text']
                            text_content += t.get('value', t) if isinstance(t, dict) else str(t)
        
        if text_content:
            return text_content
            
        # å…œåº•æ˜¾ç¤ºï¼šå¦‚æœè½®è¯¢è¶…æ—¶æˆ–è§£æå¤±è´¥
        if hasattr(final_response, 'model_dump_json'):
            return f"âš ï¸ æŠ¥å‘Šç”Ÿæˆè¶…æ—¶æˆ–è§£æå¤±è´¥ã€‚åŸå§‹ JSONï¼š\n\n{final_response.model_dump_json(indent=2, ensure_ascii=False)}"
        
        return f"âš ï¸ æ— æ³•è·å–æŠ¥å‘Šå†…å®¹ã€‚çŠ¶æ€: {getattr(final_response, 'status', 'unknown')}"

    except AttributeError as ae:
        return f"âŒ OpenAI åº“ç‰ˆæœ¬ä¸æ”¯æŒæ­¤æ“ä½œæˆ– API ç»“æ„å·²å˜æ›´: {str(ae)}"
    except Exception as e:
        return f"âŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

# ... (ä¸­é—´ä»£ç ä¿æŒä¸å˜) ...

# ============================
# STREAMLIT UI
# ============================
st.title("AuraInsight Â· å•†åœˆä¸å¢é•¿åˆ†æç³»ç»Ÿ")

# 1. æœç´¢ä¸é€‰æ‹©
address_input = st.text_input("è¯·è¾“å…¥é¤å…åœ°å€", placeholder="ä¾‹å¦‚ï¼š2406 19th Ave, San Francisco")

if address_input:
    # æœç´¢é€»è¾‘
    if "last_query" not in st.session_state or st.session_state.last_query != address_input:
        # è‡ªåŠ¨è¿½åŠ  "restaurant" ä»¥ç¡®ä¿æœç´¢çš„æ˜¯å•†å®¶è€Œä¸æ˜¯çº¯åœ°å€
        search_query = f"{address_input} restaurant"
        st.session_state.search_results = google_search(search_query)
        st.session_state.last_query = address_input
    
    results = st.session_state.get("search_results", [])

    # é”™è¯¯å¤„ç†é€»è¾‘
    if not results:
        st.warning("æœªæ‰¾åˆ°åŒ¹é…çš„é¤å…ï¼Œè¯·å°è¯•æ›´è¯¦ç»†çš„åœ°å€ã€‚")
    elif isinstance(results[0], dict) and "error" in results[0]:
        st.error(results[0]["error"])
    else:
        options = [f"{r['name']} | {r['formatted_address']}" for r in results]
        # ä½¿ç”¨ key ä¿æŒ selectbox çŠ¶æ€
        idx = st.selectbox("è¯·ç¡®è®¤åŒ¹é…çš„å•†å®¶", range(len(options)), format_func=lambda i: options[i], key="selected_idx")
        
        if idx is not None:
            place = results[idx]
            
            # 2. ç¡®è®¤æŒ‰é’®ä¸åŠ¨æ€è¿›åº¦æ¡
            if st.button("ğŸš€ ç¡®è®¤å¹¶å¼€å§‹åˆ†æå•†å®¶æ•°æ®"):
                progress_bar = st.progress(0, text="æ­£åœ¨åˆå§‹åŒ–åˆ†æ...")
                
                try:
                    lat = place["geometry"]["location"]["lat"]
                    lng = place["geometry"]["location"]["lng"]
                    
                    # æ­¥éª¤ 1: Yelp & Google è¯„è®º
                    progress_bar.progress(25, text="æ­£åœ¨è·å– Yelp ä¸ Google è¯„è®ºæ•°æ®...")
                    yelp_data = yelp_match(place["name"], lat, lng)
                    
                    # 1.1: è·å–è¯„è®ºä¸æƒ…æ„Ÿåˆ†æ
                    yelp_reviews = []
                    google_reviews = []
                    
                    # è·å– Google è¯„è®º
                    try:
                        google_reviews = get_google_reviews(place["place_id"])
                    except Exception:
                        pass

                    # è·å– Yelp è¯„è®ºä¸è¯¦æƒ…
                    yelp_details = {}
                    if yelp_data:
                        try:
                            first_biz_id = yelp_data[0]['id']
                            yelp_reviews = get_yelp_reviews(first_biz_id)
                            # è·å–å•†å®¶è¯¦æƒ…ä»¥æ‹‰å–æ›´å¤šå›¾ç‰‡
                            yelp_details = get_yelp_details(first_biz_id)
                        except Exception:
                            pass
                    
                    # åˆå¹¶è¯„è®ºè¿›è¡Œåˆ†æ
                    # æ³¨æ„ï¼šGoogle è¯„è®ºå¯¹è±¡ä¹Ÿæœ‰ 'text' å­—æ®µï¼Œä¸ Yelp ç»“æ„å…¼å®¹
                    all_reviews = google_reviews + yelp_reviews
                    sentiment_result = analyze_sentiment(all_reviews)
                    
                    # æ­¥éª¤ 2: å¤©æ°”
                    progress_bar.progress(50, text="æ­£åœ¨è·å–å†å²ä¸é¢„æµ‹å¤©æ°”æ•°æ®...")
                    weather_hist = get_weather(lat, lng)
                    noaa = noaa_forecast(lat, lng)
                    
                    # æ­¥éª¤ 3: äººå£æ™®æŸ¥
                    progress_bar.progress(75, text="æ­£åœ¨æŸ¥è¯¢å•†åœˆäººå£æ™®æŸ¥æ•°æ®...")
                    census = census_data(lat, lng)
                    
                    # å®Œæˆ
                    st.session_state.fetched_data = {
                        "place": place,
                        "yelp": yelp_data,
                        "yelp_details": yelp_details,
                        "yelp_reviews": yelp_reviews,
                        "google_reviews": google_reviews,
                        "sentiment": sentiment_result,
                        "weather_history": weather_hist.tail(10).to_dict(),
                        "noaa_forecast": noaa,
                        "census": census
                    }
                    st.session_state.current_place_id = place["place_id"]
                    
                    # æ¸…é™¤æ—§çš„æ·±åº¦æŠ¥å‘Š
                    if "report_content" in st.session_state:
                        del st.session_state.report_content
                        
                    progress_bar.progress(100, text="æ•°æ®æ‹‰å–å®Œæˆï¼")
                    
                except Exception as e:
                    st.error(f"æ•°æ®æ‹‰å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                    progress_bar.empty()

            # 3. æ˜¾ç¤ºå•†å®¶æ¦‚è¦ (ä»…å½“æ•°æ®å·²æ‹‰å–æ—¶æ˜¾ç¤º)
            if "fetched_data" in st.session_state and st.session_state.current_place_id == place["place_id"]:
                data = st.session_state.fetched_data
                
                st.divider()
                st.subheader("ğŸ“Š å•†å®¶æ•°æ®æ¦‚è¦")
                
                # Photos Gallery
                all_photo_urls = []
                
                # Google Photos (Up to 6)
                if "photos" in place:
                    for photo in place["photos"][:6]:
                        url = get_google_photo_url(photo.get("photo_reference"), max_width=800)
                        if url:
                            all_photo_urls.append(("Google", url))
                
                # Yelp Photos
                if data.get("yelp_details") and "photos" in data["yelp_details"]:
                    for url in data["yelp_details"]["photos"]:
                        all_photo_urls.append(("Yelp", url))
                
                if all_photo_urls:
                    st.markdown("#### ğŸ“¸ é—¨åº—å®æ™¯ä¸èœå“é¢„è§ˆ")
                    # ä½¿ç”¨ 3 åˆ—ç½‘æ ¼å±•ç¤ºå›¾ç‰‡
                    cols = st.columns(3)
                    for i, (source, url) in enumerate(all_photo_urls):
                        with cols[i % 3]:
                            st.image(url, caption=f"æ¥æº: {source}", use_column_width=True)
                                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.info(f"**Google è¯„åˆ†**: {place.get('rating', 'N/A')} ({place.get('user_ratings_total', 0)} æ¡)")
                with col2:
                    yelp_rating = data['yelp'][0]['rating'] if data['yelp'] else "N/A"
                    yelp_count = data['yelp'][0]['review_count'] if data['yelp'] else 0
                    st.error(f"**Yelp è¯„åˆ†**: {yelp_rating} ({yelp_count} æ¡)")
                with col3:
                    st.success(f"**å•†åœˆäººå£ (3è‹±é‡Œ)**: {data['census']['population_est']}")

                # Sentiment
                if data.get("sentiment"):
                    st.markdown("#### ğŸ’¬ è¯„è®ºæƒ…æ„Ÿæ´å¯Ÿ")
                    sent = data["sentiment"]
                    s_col1, s_col2 = st.columns([1, 2])
                    with s_col1:
                        st.metric("æƒ…æ„Ÿå€¾å‘", sent.get("label", "N/A"), f"{sent.get('score', 0):.2f}")
                    with s_col2:
                        if sent.get("keywords"):
                            st.write("**çƒ­é—¨å…³é”®è¯:**")
                            st.write(" ".join([f"`{k}`" for k in sent["keywords"]]))
                        else:
                            st.write("æš‚æ— è¶³å¤Ÿè¯„è®ºæå–å…³é”®è¯")

                with st.expander("æŸ¥çœ‹è¯¦ç»†åŸå§‹æ•°æ®"):
                    st.json(data)

                st.divider()

                # 4. æ·±åº¦åˆ†ææŒ‰é’®
                col_btn, col_lang = st.columns([1, 1])
                with col_lang:
                    lang = st.selectbox("æŠ¥å‘Šè¯­è¨€", ["zh", "en"], key="report_lang")
                
                with col_btn:
                    if st.button("ğŸ” ç”Ÿæˆæ·±åº¦AIç­–ç•¥æŠ¥å‘Š", type="primary"):
                        # åˆå§‹åŒ–è¿›åº¦æ¡
                        report_progress = st.progress(0, text="æ­£åœ¨å¯åŠ¨ AI å¼•æ“...")
                        
                        try:
                            # é˜¶æ®µ 1: å‡†å¤‡ä¸Šä¸‹æ–‡
                            report_progress.progress(10, text="æ­£åœ¨æ•´åˆå•†å®¶æ•°æ®ä¸å•†åœˆä¿¡æ¯...")
                            time.sleep(0.5)
                            
                            # é˜¶æ®µ 2: æ„å»º Prompt
                            report_progress.progress(30, text="æ­£åœ¨æ„å»ºé«˜ç»´åˆ†ææ¨¡å‹...")
                            
                            # é˜¶æ®µ 3: å¼‚æ­¥è°ƒç”¨ API å¹¶åŠ¨æ€æ›´æ–°æ–‡å­—
                            loading_texts = [
                                "æ­£åœ¨é€šè¿‡ GPT-4o è¿›è¡Œæ·±åº¦æ¨ç†...",
                                "æ­£åœ¨åˆ†æ Yelp ä¸ Google è¯„è®ºæƒ…æ„Ÿè¶‹åŠ¿...",
                                "æ­£åœ¨ç»“åˆ Meteostat å†å²å¤©æ°”æ•°æ®è¿›è¡Œå›å½’åˆ†æ...",
                                "æ­£åœ¨äº¤å‰æ¯”å¯¹ Census å•†åœˆäººå£ç»Ÿè®¡æ•°æ®...",
                                "æ­£åœ¨ç”Ÿæˆéº¦è‚¯é”¡é£æ ¼çš„æˆ˜ç•¥å»ºè®®...",
                                "æ­£åœ¨ä¼˜åŒ–æŠ¥å‘Šæ ¼å¼ä¸æ’ç‰ˆ..."
                            ]
                            
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(generate_report, data, lang)
                                
                                # å¾ªç¯æ›´æ–°è¿›åº¦æ¡æ–‡å­—ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆ
                                idx = 0
                                progress_val = 30
                                while not future.done():
                                    if idx < len(loading_texts):
                                        current_text = loading_texts[idx]
                                    else:
                                        # å¦‚æœæ‰€æœ‰é¢„è®¾æ–‡æ¡ˆéƒ½æ˜¾ç¤ºå®Œäº†ï¼Œä¸å†å¾ªç¯ï¼Œè€Œæ˜¯æ˜¾ç¤ºé€šç”¨ç­‰å¾…æç¤º
                                        current_text = "æ­£åœ¨è¿›è¡Œæœ€ç»ˆçš„æ·±åº¦é€»è¾‘æ•´åˆï¼Œè¯·è€å¿ƒç­‰å¾…..."
                                    
                                    # è®©è¿›åº¦æ¡ç¼“æ…¢å¢åŠ ï¼Œä½†ä¸åˆ° 100%
                                    if progress_val < 90:
                                        progress_val += 1
                                    
                                    report_progress.progress(progress_val, text=f"AI é¡¾é—®å·¥ä½œæµ: {current_text}")
                                    time.sleep(1.5) # æ¯ 1.5 ç§’åˆ‡æ¢ä¸€æ¬¡æ–‡å­—
                                    idx += 1
                                
                                # è·å–ç»“æœ
                                report = future.result()
                            
                            # é˜¶æ®µ 4: å¤„ç†å“åº”
                            report_progress.progress(95, text="æ­£åœ¨æœ€ç»ˆæ ¼å¼åŒ–æŠ¥å‘Šå†…å®¹...")
                            st.session_state.report_content = report
                            
                            # å®Œæˆ
                            report_progress.progress(100, text="æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼")
                            
                        except Exception as e:
                            report_progress.empty()
                            st.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            
            # 5. å¯ç¼–è¾‘æŠ¥å‘Šä¸å¯¼å‡º
            if "report_content" in st.session_state and st.session_state.current_place_id == place["place_id"]:
                st.divider()
                st.subheader("ğŸ“ æ·±åº¦åˆ†ææŠ¥å‘Š (å¯ç¼–è¾‘)")
                
                # ç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æŠ¥å‘Š
                user_edited_report = st.text_area(
                    "æ‚¨å¯ä»¥ç›´æ¥ä¿®æ”¹ä¸‹æ–¹çš„æŠ¥å‘Šå†…å®¹ï¼Œä¿®æ”¹åç‚¹å‡»ä¸‹è½½å³å¯ã€‚",
                    value=st.session_state.report_content,
                    height=500,
                    key="report_area"
                )
                
                # å¯¼å‡ºæŒ‰é’®
                col_exp1, col_exp2 = st.columns([1, 1])
                with col_exp1:
                    if st.button("ğŸ“¥ å¯¼å‡º PDF åˆ†ææŠ¥å‘Š"):
                        export_pdf(user_edited_report, "analysis_report.pdf")
                        with open("analysis_report.pdf", "rb") as pdf_file:
                            st.download_button(
                                label="ç‚¹å‡»ä¸‹è½½ PDF",
                                data=pdf_file,
                                file_name="AuraInsight_Report.pdf",
                                mime="application/pdf"
                            )
                        st.success("PDF å·²ç”Ÿæˆï¼")

                # ============================
                # é˜¶æ®µ 1.2: è¡¥å……æ•°æ®ä¸Šä¼ åŒºåŸŸ (é—­ç¯æ ¸å¿ƒ)
                # ============================
                if ENABLE_DATA_UPLOAD_PIPELINE:
                    st.divider()
                    st.markdown("### ğŸ“Š è¡¥å……è¿è¥æ•°æ®ï¼ˆæ•°æ®é—­ç¯ï¼‰")
                    st.info("ğŸ’¡ ä¸Šä¼ çœŸå®æ•°æ®ï¼ˆPOS/å¤–å–å¹³å°å¯¼å‡ºï¼‰åï¼ŒAI å°†é‡æ–°æ¸…æ´—å¹¶æ ¡å‡†æ¨¡å‹ç»“è®ºï¼Œæä¾›æ›´é«˜ç²¾åº¦çš„æŠ¥å‘Šã€‚")
                    
                    uploaded_files = st.file_uploader(
                        "æ”¯æŒ CSV, XLSX, TXT (æ”¯æŒå¤šæ–‡ä»¶åŒæ—¶ä¸Šä¼ )", 
                        accept_multiple_files=True,
                        type=['csv', 'xlsx', 'xls', 'txt']
                    )
                    
                    if uploaded_files:
                        bundle, dfs = DataPipeline.process_bundle(uploaded_files)
                        
                        # ä¸‰å—å¯è§†åŒ–ï¼šå·²è¯†åˆ«ã€ç¼ºå¤±ã€å‡è®¾
                        v_col1, v_col2, v_col3 = st.columns(3)
                        with v_col1:
                            st.success("**å·²è¯†åˆ«å­—æ®µ**")
                            for f in bundle["verified"].keys(): st.write(f"âœ… {f}")
                        with v_col2:
                            st.warning("**ç¼ºå¤±å­—æ®µ**")
                            all_needed = ['orders', 'revenue', 'aov', 'cancel_rate', 'prep_time']
                            missing = [f for f in all_needed if f not in bundle["verified"] and f not in bundle["derived"]]
                            for f in missing: st.write(f"â“ {f}")
                        with v_col3:
                            st.info("**å°†é‡‡ç”¨çš„æ¨¡å‹å‡è®¾**")
                            for f in missing: st.write(f"ğŸ”® {f} (Industry Prior)")
                        
                        # æŒ‰é’®é€»è¾‘
                        c_btn1, c_btn2 = st.columns(2)
                        with c_btn1:
                            if st.button("ğŸ” è§£æå¹¶é¢„è§ˆæ•°æ®å†…å®¹"):
                                for d in dfs: st.dataframe(d.head(5))
                                
                        with c_btn2:
                            if st.button("ğŸ”„ ä½¿ç”¨ä¸Šä¼ æ•°æ®é‡æ–°ç”ŸæˆæŠ¥å‘Š", type="primary"):
                                with st.progress(0, text="æ­£åœ¨å¯åŠ¨æ•°æ®å¢å¼ºç®¡çº¿..."):
                                    # æ„é€ ä¸Šä¼ æ•°æ®çš„åˆ†æ schema
                                    op_data = {
                                        "traceability": bundle["traceability"],
                                        "sample_metrics": bundle["verified"]
                                    }
                                    new_report = generate_report(data, lang, operational_data=op_data)
                                    st.session_state.report_content = new_report
                                    st.rerun()

                    # Admin å›æ»šå¼€å…³ (éšè—)
                    if st.toggle("Admin: ä½¿ç”¨æ—§æ¨¡å‹ç‰ˆæœ¬ (Rollback Mode)", value=False):
                        st.session_state.use_legacy_model = True
                    else:
                        st.session_state.use_legacy_model = False

